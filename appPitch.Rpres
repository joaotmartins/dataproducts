Comparing prediction algorithms over the 'iris' dataset
========================================================
author: Jo√£o Martins
date: 2016-06-03

Overview
========================================================

* A small application that compares how three different algorithms perform on a 
classification task.

* Classification is done over the `iris` dataset.


Algorithms and Parameters
========================================================

Users can choose between three different algorithms supported by 
the `caret` package:

* Logistic regression - using the `multinom` algorithm
* Random forests - `rf`
* K-Nearest-Neighbors - `knn`

Besides the algorithms, users can change:

* The number of resampling iterations
* The split between training and testing on the `iris`dataset


Output (1/2)
========================================================

The output is a confusion matrix, prediction accuracy, and a plot over the two
most important prediction variables.

An example using random forests, with 5 resampling iterations and a 60/40 
training/testing will yield the following:

```{r, echo=FALSE}

library(caret)
library(e1071)
library(pROC)
library(randomForest)
library(dplyr)

modelFit <- function(classif.method, boost.number = 25, pct.train = 70) {
        ret <- list()
        
        set.seed(1234567)
        
        inTrain <- createDataPartition(iris$Species, p = (pct.train / 100))[[1]]
        training <- iris[inTrain, ]
        testing <- iris[-inTrain, ]
        
        m <- classif.method
        
        tc <- trainControl(savePredictions = "final", number = boost.number)
        
        fit <- train(Species ~ ., data = training, method = m, trControl = tc)
        
        preds <- predict(fit, newdata = testing)
        d <- data.frame(Prediction = preds, Actual = testing$Species)
        
        ret$preds <- preds
        ret$actual <- testing
        ret$model <- fit
        ret$resTable <- table(d)
        ret$confMatrix <- confusionMatrix(preds, testing$Species)
        
        ret
} 

# gets the two most important variables of the currently calculated model
getMostImportantVars <- function(currModel) {
        importance <- varImp(currModel$model)$importance
        col.desc <- paste0("desc(", colnames(importance), ")")
        
        imp <- add_rownames(importance, var = "varname") %>% 
                arrange_(.dots = col.desc) %>%
                filter(row_number() <= 2)
        
        imp
}

# Plot predictions vs. actual values; plot is made by the two most important
# classification variables.
accuracyPlot <- function(impVars, predictions, actual) {
        
        d <- cbind(actual, prediction = predictions)
        d <- mutate(d, Correct = Species == prediction)
        
        ggplot(d, aes_string(x = impVars$varname[1], y = impVars$varname[2])) + 
                geom_point(aes(shape = Correct, color = Species, size = 50)) +
                guides(shape = "legend", color = "legend", size = "none") +
                ggtitle("Test values, plotted over the two most significant classification variables")
}

# render plot predictions based on currently calculated model
makePlot <- function(currModel) {
        if (! is.null(currModel)) {
                iv <- getMostImportantVars(currModel)
                accuracyPlot(iv, currModel$preds, currModel$actual)
        }
}

currentModel <- modelFit("rf", 5, 60)
currentModel$resTable
currentModel$confMatrix$overall[1]
```

Output (2/2)
==============================================

```{r, echo=FALSE}
makePlot(currentModel)

```
